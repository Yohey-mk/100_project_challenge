# decision_tree_and_random_forest.py

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Titanic dataã‚’å–å¾—
df = fetch_openml("titanic", version=1, as_frame=True).frame

# "survived"ã‚’ç›®çš„å¤‰æ•°ã«ã™ã‚‹
y = df["survived"]
X = df.drop("survived", axis=1)

# Category -> one-hot
X = pd.get_dummies(X, drop_first=True)

# æ¬ æå€¤ã¯ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆRandomForestã¯ã“ã‚Œã§OKï¼‰
X = X.fillna(X.median())

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# æ±ºå®šæœ¨ã§åˆ†é¡
tree = DecisionTreeClassifier(max_depth=5, random_state=42)
tree.fit(X_train, y_train)

y_pred_tree = tree.predict(X_test)
acc_tree = accuracy_score(y_test, y_pred_tree)
print("Decision Tree Accuracy: ", acc_tree)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§åˆ†é¡
forest = RandomForestClassifier(
    n_estimators=100,
    random_state=42
)
forest.fit(X_train, y_train)

y_pred_forest = forest.predict(X_test)
acc_forest = accuracy_score(y_test, y_pred_forest)
print("Random Forest Accuracy:", acc_forest)

# é‡è¦ãªç‰¹å¾´é‡ã‚’å¯è¦–åŒ–
importances = forest.feature_importances_
indices = np.argsort(importances[::-1])

plt.figure(figsize=(10, 6))
plt.bar(range(10), importances[indices][:10])
plt.xticks(range(10), X.columns[indices][:10], rotation=45)
plt.title("Top 10 Important Features")
plt.tight_layout()
plt.show()

# æ±ºå®šæœ¨ã®æ·±ã•ã¨ç²¾åº¦ã®é–¢ä¿‚ *ã“ã‚Œã‚’matplotlibã§æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã™ã‚‹ã€‚â­ï¸ToDo
depths = range(1, 20)
scores = []

for d in depths:
    model = DecisionTreeClassifier(max_depth=d, random_state=42)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    scores.append(accuracy_score(y_test, pred))

plt.figure(figsize=(8, 5))
plt.plot(depths, scores, marker="o")
plt.xlabel("Depth of Decision Tree")
plt.ylabel("Accuracy")
plt.title("Decision Tree: Depth vs Accuracy")
plt.grid(True)
plt.tight_layout()
plt.show()

# Notes
#ğŸ§  å­¦ã¹ã‚‹ã“ã¨
#	â€¢	æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆå‰å‡¦ç†â†’å­¦ç¿’â†’è©•ä¾¡ï¼‰
#	â€¢	æ±ºå®šæœ¨ã®ã€Œæœ¨æ§‹é€ ã€ã®æ„å‘³
#	â€¢	ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ä»•çµ„ã¿ï¼ˆè¤‡æ•°ã®æœ¨ã§æŠ•ç¥¨ï¼‰
#	â€¢	ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹
#	â€¢	ç‰¹å¾´é‡ã®é‡è¦åº¦ã®è¦‹æ–¹
#ğŸ§  è¿½åŠ ã®å­¦ã³ãƒã‚¤ãƒ³ãƒˆï¼ˆã§ãã‚Œã°å¾Œã§ã‚„ã£ã¦ã¿ã¦ï¼‰
#âœ” 1. train accuracy ã‚‚åŒæ™‚ã«æãã¨éå­¦ç¿’ãŒè¦‹ãˆã‚‹
#æ·±ã„æœ¨ã»ã©ã€Œè¨“ç·´ç²¾åº¦100%ã€ã«ãªã‚‹ã®ã§ã€å¼·çƒˆã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚
#âœ” 2. ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã® n_estimators ã‚’å¤‰ãˆã¦åŒã˜ã‚°ãƒ©ãƒ•ã‚’ä½œã‚‹
#ã“ã¡ã‚‰ã‚‚æ·±ã„ç†è§£ã«å½¹ç«‹ã¡ã¾ã™ã€‚